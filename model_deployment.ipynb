{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_deployment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFOw6h1vtscg"
      },
      "source": [
        "from random import randint\n",
        "from pickle import load\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G74MdP_IyMk_"
      },
      "source": [
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "    result = []\n",
        "    in_text = seed_text\n",
        "    # generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "        # encoded the text as integer\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # predict the probability for each word\n",
        "        yhat = np.argmax(model.predict(encoded, verbose=0)).item()\n",
        "        # map predicted word index to word\n",
        "        out_word = ''\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            # print(type(index))\n",
        "            if index == yhat:\n",
        "                out_word = word\n",
        "                break\n",
        "        in_text += ' '+out_word\n",
        "        result.append(out_word)\n",
        "    return ' '.join(result)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmPYErMw86TL"
      },
      "source": [
        "# load the model\n",
        "\n",
        "one_gram_model = ''\n",
        "two_gram_model = load_model('/content/drive/MyDrive/model/tow_gram_model.h5')\n",
        "three_gram_model = load_model('/content/drive/MyDrive/model/three_gram_model.h5')\n",
        "four_gram_model = load_model('/content/drive/MyDrive/model/four_gram_model.h5')\n",
        "five_gram_model = load_model('/content/drive/MyDrive/model/five_gram_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHmbQ4rB9uus"
      },
      "source": [
        "# load the tokenizer \n",
        "\n",
        "one_gram_tokenizer = ''\n",
        "two_gram_tokenizer = load(open('/content/drive/MyDrive/model/two_gram_tokenizer.pkl', 'rb'))\n",
        "three_gram_tokenizer = load(open('/content/drive/MyDrive/model/three_gram_tokenizer.pkl', 'rb'))\n",
        "four_gram_tokenizer = load(open('/content/drive/MyDrive/model/tokenizer_four_gram.pkl', 'rb'))\n",
        "five_gram_tokenizer = load(open('/content/drive/MyDrive/model/tokenizer_5_1.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icshFQgW_Tvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76a58a2f-2f3f-4da8-c565-df9593fe9e3c"
      },
      "source": [
        "# seed_text = 'কর্মজীবন থেকে অবসরের বিষয়টিকে'\n",
        "s=time.time()\n",
        "seed_text = 'আইডি কার্ড ধারণ'\n",
        "res = len(seed_text.split())\n",
        "if res>5:\n",
        "    res = 3\n",
        "    seed_text.split()\n",
        "    lent = lent-3\n",
        "    seed_text = seed_text[lent:]\n",
        "    seed_text = ' '.join(map(str, seed_text))\n",
        "\n",
        "seq_length = res\n",
        "\n",
        "if res == 2:\n",
        "    model = two_gram_model\n",
        "    tokenizer = two_gram_tokenizer\n",
        "\n",
        "elif res == 3:\n",
        "    model_new = two_gram_model\n",
        "    tokenizer_new = two_gram_tokenizer\n",
        "\n",
        "    model = three_gram_model\n",
        "    tokenizer = three_gram_tokenizer\n",
        "elif res==4:\n",
        "    model_new = three_gram_model\n",
        "    tokenizer_new = three_gram_tokenizer\n",
        "\n",
        "    model = four_gram_model\n",
        "    tokenizer = four_gram_tokenizer\n",
        "\n",
        "else:\n",
        "    model_new = four_gram_model\n",
        "    tokenizer_new = four_gram_tokenizer\n",
        "\n",
        "    model = five_gram_model\n",
        "    tokenizer = five_gram_tokenizer\n",
        "\n",
        "print('inputed words: ' + '\\n' + seed_text +'\\n')\n",
        "print('predicted next word: ')\n",
        "generated_word1 = generate_seq(model, tokenizer, seq_length, seed_text, 1)\n",
        "print(generated_word1)\n",
        "# for i in range(1,3):\n",
        "#     generated_word1 = generate_seq(model, tokenizer, seq_length, seed_text, i+1)\n",
        "#     print(generated_word1, end=' ')\n",
        "#     print('\\n')\n",
        "print(time.time()-s)\n",
        "\n",
        "#predicted full sentence = 1\n",
        "\n",
        "# generated_pred_sen_1 = []\n",
        "# generated_all_sen_1 = generate_seq(model, tokenizer, seq_length, seed_text, 15)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputed words: \n",
            "আইডি কার্ড ধারণ\n",
            "\n",
            "predicted next word: \n",
            "করুন\n",
            "0.7565696239471436\n"
          ]
        }
      ]
    }
  ]
}